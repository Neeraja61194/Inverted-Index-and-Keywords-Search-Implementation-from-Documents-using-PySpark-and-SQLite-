{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Create_Inverted_Index From Text.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V7CqdE1VhI8_"
      },
      "source": [
        "# The first part of the project:\n",
        "### 1. Create an Inverted Index from .Html Documents :\n",
        "\n",
        "Reads the documents (.html files) as text files.\n",
        "\n",
        "Cleans all the documents using the NLTK library - Convert to lowercase, remove stopwords, punctuations, special characters, numbers etc. \n",
        " \n",
        "Calculates the TF-IDF (Term frequency - Inverse Document Frequency) and the document magnitude\n",
        "\n",
        "Save the inverted index and magnitude in text files.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UWWk2ywvc-7n"
      },
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-2.4.4/spark-2.4.4-bin-hadoop2.7.tgz\n",
        "!tar xf spark-2.4.4-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark\n",
        "\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.4-bin-hadoop2.7\"\n",
        "\n",
        "import findspark\n",
        "findspark.init(\"spark-2.4.4-bin-hadoop2.7\")# SPARK_HOME\n",
        "\n",
        "from pyspark import SparkContext\n",
        "sc = SparkContext.getOrCreate()"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4wvdXQxh9m2f",
        "outputId": "be207368-0327-4714-d667-5c962a40fbbb"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tFQa1qU-Ml-B",
        "outputId": "bcc204e5-c154-425f-d363-1d5f5235276b"
      },
      "source": [
        "# Upload a zip file \"input_docs_sample.zip\" containing the .html documents to Google Drive\n",
        "\n",
        "!rm -rf input_docs\n",
        "!cp /content/drive/MyDrive/input_docs_sample.zip .\n",
        "!unzip input_docs_sample.zip > /dev/null\n",
        "!ls input_docs/ | wc -l\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SW0lXYo6Hqso",
        "outputId": "c814d2c2-a91d-4f31-c690-8c0625ad47ed"
      },
      "source": [
        "# Importing the NLTK package for text processing\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "usmMrr5Uhipf"
      },
      "source": [
        "**Creating an RDD from a text file**\n",
        "\n",
        "Each line of the text file becomes an element of the RDD."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TVfPs26GjAal",
        "outputId": "2e7ddf60-0c8f-4db8-81d4-ba2326f8990e"
      },
      "source": [
        "# wholeTextFiles generates an RDD of pair values, \n",
        "# where the key is the full path of each file, the value is the content of each file\n",
        "\n",
        "input = sc.wholeTextFiles(\"input_docs\");\n",
        "\n",
        "# Stripping the path of the file and obtain only the basename. \n",
        "# e.g. 'file:/content/drive/My Drive/Colab Notebooks/data_spark/input_docs/1.html'\n",
        "# becomes '1.html' \n",
        "import os\n",
        "\n",
        "# Maps the document id with the corresponding text --> (did,text)\n",
        "input2 = input.map(lambda x: (int(os.path.basename(x[0]).split(\".\")[0]), x[1])) # in input 2 the text in document is stored as key value pair\n",
        "\n",
        "print(input2.take(1))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(4, '<H2>26-FEB-1987 15:07:13.72</H2>\\r\\n<H2>TALKING POINT/BANKAMERICA BAC EQUITY OFFER</H2>\\r\\nBankAmerica Corp is not under\\npressure to act quickly on its proposed equity offering and\\nwould do well to delay it because of the stock\\'s recent poor\\nperformance, banking analysts said.\\n    Some analysts said they have recommended BankAmerica delay\\nits up to one-billion-dlr equity offering, which has yet to be\\napproved by the Securities and Exchange Commission.\\n    BankAmerica stock fell this week, along with other banking\\nissues, on the news that Brazil has suspended interest payments\\non a large portion of its foreign debt.\\n    The stock traded around 12, down 1/8, this afternoon,\\nafter falling to 11-1/2 earlier this week on the news.\\n    Banking analysts said that with the immediate threat of the\\nFirst Interstate Bancorp I takeover bid gone, BankAmerica is\\nunder no pressure to sell the securities into a market that\\nwill be nervous on bank stocks in the near term.\\n    BankAmerica filed the offer on January 26. It was seen as\\none of the major factors leading the First Interstate\\nwithdrawing its takeover bid on February 9.\\n    A BankAmerica spokesman said SEC approval is taking longer\\nthan expected and market conditions must now be re-evaluated.\\n    \"The circumstances at the time will determine what we do,\"\\nsaid Arthur Miller, BankAmerica\\'s Vice President for Financial\\nCommunications, when asked if BankAmerica would proceed with\\nthe offer immediately after it receives SEC approval.\\n    \"I\\'d put it off as long as they conceivably could,\" said\\nLawrence Cohn, analyst with Merrill Lynch, Pierce, Fenner and\\nSmith.\\n    Cohn said the longer BankAmerica waits, the longer they\\nhave to show the market an improved financial outlook.\\n    Although BankAmerica has yet to specify the types of\\nequities it would offer, most analysts believed a convertible\\npreferred stock would encompass at least part of it.\\n    Such an offering at a depressed stock price would mean a\\nlower conversion price and more dilution to BankAmerica stock\\nholders, noted Daniel Williams, analyst with Sutro Group.\\n    Several analysts said that while they believe the Brazilian\\ndebt problem will continue to hang over the banking industry\\nthrough the quarter, the initial shock reaction is likely to\\nease over the coming weeks.\\n    Nevertheless, BankAmerica, which holds about 2.70 billion\\ndlrs in Brazilian loans, stands to lose 15-20 mln dlrs if the\\ninterest rate is reduced on the debt, and as much as 200 mln\\ndlrs if Brazil pays no interest for a year, said Joseph\\nArsenio, analyst with Birr, Wilson and Co.\\n    He noted, however, that any potential losses would not show\\nup in the current quarter.\\n    With other major banks standing to lose even more than\\nBankAmerica if Brazil fails to service its debt, the analysts\\nsaid they expect the debt will be restructured, similar to way\\nMexico\\'s debt was, minimizing losses to the creditor banks.\\n \\n ')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AdFPY9uqgq1P",
        "outputId": "9831e782-de94-4c92-d8b6-64f121102db1"
      },
      "source": [
        "# Doc to wordlist function\n",
        "# The output will be a list of tuples such as \n",
        "# (\"search\", (4,15,15/20)), \n",
        "# where 4 is docid, \n",
        "# 15 is frequency of \"search\" in this doc, \n",
        "# 20 is maxf in in the document.\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "from collections import Counter\n",
        "import re\n",
        "import string\n",
        "from string import digits\n",
        "punctuations = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~'''\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Function for creating a list of tuples of the form (word, (docid, freq, freq/maxfreq)) when docid and text is given. \n",
        "def dw(docid, htmltext):\n",
        "  htmltext_lower = htmltext.lower() #converting to lower case\n",
        "  cleantext_notags = BeautifulSoup(htmltext_lower).get_text() #removing html tags\n",
        "  remove_digits = str.maketrans('','',digits)  #To find didgits in the text and then removing in next line\n",
        "  cleantext_nodigits = cleantext_notags.translate(remove_digits) #removing numbers \n",
        "  cleantext_nosplchar = re.sub('[^A-Za-z0-9]+',' ',cleantext_nodigits) #remove special characters\n",
        "  cleantext_nopunct = ''\n",
        "  for char in cleantext_nosplchar:\n",
        "    if char not in punctuations:\n",
        "      cleantext_nopunct = cleantext_nopunct + char #remove punctuations\n",
        "\n",
        "  cleantext_tokenize = word_tokenize(cleantext_nopunct) #tokenized text for getting this us an individual word\n",
        "  \n",
        "  cleantext_nostopwords = [word for word in cleantext_tokenize if word not in stopwords.words('english')] #removing stopwords\n",
        "  \n",
        "  counter = Counter(cleantext_nostopwords) #counting the number of words and it is stored as a key value pair where key is the word and value is frequency\n",
        "\n",
        "  maxfrequency_word = counter.most_common(1)[0][1] #finding maximum number of times a word is getting repeated in the document\n",
        "  output = []\n",
        "  for obj in counter: # creating required format for rdd\n",
        "    key = (obj)\n",
        "    value = (int(docid), counter[obj], counter[obj]/maxfrequency_word) #here docid ,freq and term freq(tf) is stored\n",
        "    map = (key,value) \n",
        "    output.append(map) #adding to output\n",
        "  return output\n",
        "\n",
        "#dw(2,\"<H2>26-FEB-1987 15:01:01.79</H2><H2>BAHIA COCOA REVIEW</H2>Showers continued throughout the week in the Bahia cocoa zone, alleviating the drought since early January and improving prospects for the coming temporao, although normal humidity levels have not been restored, Comissaria Smith said in its weekly review. The dry period means the temporao will be late this year. Arrivals for the week ended February 22 were 155,221 bags of 60 kilos making a cumulative total for the season of 5.93 mln against 5.81 at the same stage last year. Again it seems that cocoa delivered earlier on consignment was included in the arrivals figures. Comissaria Smith said there is still some doubt as to how much old crop cocoa is still available as harvesting has practically come to an end. With total Bahia crop estimates around 6.4 mln bags and sales standing at almost 6.2 mln there are a few hundred thousand bags still in the hands of farmers, middlemen, exporters and processors. There are doubts as to how much of this cocoa would be fit for export as shippers are now experiencing dificulties in obtaining +Bahia superior+ certificates. In view of the lower quality over recent weeks farmers have sold a good part of their cocoa held on consignment. Comissaria Smith said spot bean prices rose to 340 to 350 cruzados per arroba of 15 kilos. Bean shippers were reluctant to offer nearby shipment and only limited sales were booked for March shipment at 1,750 to 1,780 dlrs per tonne to ports to be named. New crop sales were also light and all to open ports with June/July going at 1,850 and 1,880 dlrs and at 35 and 45 dlrs under New York july, Aug/Sept at 1,870, 1,875 and 1,880 dlrs per tonne FOB. Routine sales of butter were made. March/April sold at 4,340, 4,345 and 4,350 dlrs. April/May butter went at 2.27 times New York May, June/July at 4,400 and 4,415 dlrs, Aug/Sept at 4,351 to 4,450 dlrs and at 2.27 and 2.28 times New York Sept and Oct/Dec at 4,480 dlrs and 2.27 times New York Dec, Comissaria Smith said. Destinations were the U.S., Covertible currency areas, Uruguay and open ports. Cake sales were registered at 785 to 995 dlrs for March/April, 785 dlrs for May, 753 dlrs for Aug and 0.39 times New York Dec for Oct/Dec. Buyers were the U.S., Argentina, Uruguay and convertible currency areas. Liquor sales were limited with March/April selling at 2,325 and 2,380 dlrs, June/July at 2,375 dlrs and at 1.25 times New York July, Aug/Sept at 2,400 dlrs and at 1.25 times New York Sept and Oct/Dec at 1.25 times New York Dec, Comissaria Smith said. Total Bahia sales are currently estimated at 6.13 mln bags against the 1986/87 crop and 1.06 mln bags against the 1987/88 crop. Final figures for the period to February 28 are expected to be published by the Brazilian Cocoa Trade Commission after carnival which ends midday on February 27.\")\n",
        "word_docid_freq_tf = input2.flatMap(lambda x: dw(x[0],x[1])) # we  are creating an rdd \n",
        "print(word_docid_freq_tf.take(2))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('feb', (4, 1, 0.07142857142857142)), ('talking', (4, 1, 0.07142857142857142))]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ZiKzsibkj92",
        "outputId": "3056eee5-0ed6-4c21-8815-99ab391a74d2"
      },
      "source": [
        "# This cell creates an RDD of the form (word, [(did1,freq1,tf1), (did2,freq2,tf2), ...]) \n",
        "# ie. creates a posting list of the words\n",
        "\n",
        "word_docid_freq_tf_rdd_2 = word_docid_freq_tf.map(lambda x : (x[0],[x[1]])).reduceByKey(lambda a,b: a+b)\n",
        "word_posting_list_tf = sc.parallelize(word_docid_freq_tf_rdd_2.collect())\n",
        "print(word_posting_list_tf.count())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "396\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gtDZ4q-1oaq1",
        "outputId": "812a1e84-7292-4b88-b7ec-de5df570cb9f"
      },
      "source": [
        "print(word_posting_list_tf.map(lambda x : (x[0], list(x[1]))).take(1))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('feb', [(4, 1, 0.07142857142857142), (5, 1, 0.16666666666666666), (1, 1, 0.07142857142857142), (2, 1, 0.2), (3, 1, 0.3333333333333333)])]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y_8KGpQvlLGp",
        "outputId": "50c92b76-3a33-4514-bb28-416951163937"
      },
      "source": [
        "# This cell creates the tf-idf \n",
        "# (word, [(did,freq,tfidf), ...])\n",
        "# idf = 1/len(postinglist_tf)\n",
        "\n",
        "tf_idf = []\n",
        "for row in word_posting_list_tf.collect():\n",
        "  new_tuple_row = '' \n",
        "  word_variable = row[0]\n",
        "  idf = 1/len(row[1])\n",
        "  new_row_tuple_array = []\n",
        "  for rowindex in row[1]:\n",
        "    rowindex_tf = rowindex[2]\n",
        "    rowindex_tfidf = idf * rowindex_tf\n",
        "    new_tuple_rowindex = ''\n",
        "    new_tuple_rowindex = (rowindex[0],rowindex[1],rowindex_tfidf)\n",
        "    new_row_tuple_array.append(new_tuple_rowindex)\n",
        "  new_tuple_row = (word_variable,new_row_tuple_array)\n",
        "  tf_idf.append(new_tuple_row)\n",
        "  word_posting_list_tf_idf = sc.parallelize(tf_idf)\n",
        "  \n",
        "print(word_posting_list_tf_idf.take(1))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('feb', [(4, 1, 0.014285714285714285), (5, 1, 0.03333333333333333), (1, 1, 0.014285714285714285), (2, 1, 0.04000000000000001), (3, 1, 0.06666666666666667)])]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U8jahG9q2SVf",
        "outputId": "6a8c8d61-121d-49c1-a31b-bf2a57667531"
      },
      "source": [
        "# For obtaining the magnitude of each document.\n",
        "\n",
        "docid_freq_tfidf_rdd3_array = []\n",
        "for row in word_posting_list_tf_idf.collect():\n",
        "  for rowindex in row[1]:\n",
        "    # Magnitude\n",
        "    new_tuple = (rowindex[1],(rowindex[2]*rowindex[2])) \n",
        "    rowindex = (rowindex[0],new_tuple)\n",
        "  docid_freq_tfidf_rdd3_array.append(rowindex)\n",
        "docid_freq_tfidf_rdd3 = sc.parallelize(docid_freq_tfidf_rdd3_array)\n",
        "new_list1 = docid_freq_tfidf_rdd3.reduceByKey(lambda x,y: (max(x[0],y[0]),(float(x[1])+float(y[1]))))\n",
        "\n",
        "doc_maxf_mag = sc.parallelize(new_list1.collect())\n",
        "\n",
        "print(doc_maxf_mag.take(2))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(4, (14, 2.7653061224489752)), (2, (5, 3.8700000000000014))]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zBByinGhlu7-"
      },
      "source": [
        "# Saving the inverted index created as a text file\n",
        "!rm -rf inv_idx\n",
        "word_posting_list_tf_idf.saveAsTextFile(\"inv_idx\");"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "szlsKC-RmWvd"
      },
      "source": [
        "# Saving the document magnitude created as a text file\n",
        "!rm -rf doc_mag\n",
        "doc_maxf_mag.saveAsTextFile(\"doc_mag\");"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vNplByM4fsvF",
        "outputId": "867db4fc-78d4-4506-e999-f49e311e3aac"
      },
      "source": [
        "!ls -lrt inv_idx\n",
        "!head inv_idx/part-00001\n",
        "!wc -l inv_idx/part-00000\n",
        "!wc -l inv_idx/part-00001\n",
        "!cat inv_idx/part-00000 inv_idx/part-00001 > /content/drive/MyDrive/inv_idx.txt\n",
        "!wc -l /content/drive/MyDrive/inv_idx.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 24\n",
            "-rw-r--r-- 1 root root 9056 Apr  8 17:00 part-00000\n",
            "-rw-r--r-- 1 root root 8543 Apr  8 17:00 part-00001\n",
            "-rw-r--r-- 1 root root    0 Apr  8 17:00 _SUCCESS\n",
            "('aug', [(1, 4, 0.2857142857142857)])\n",
            "('sept', [(1, 5, 0.35714285714285715)])\n",
            "('butter', [(1, 2, 0.14285714285714285)])\n",
            "('april', [(1, 4, 0.2857142857142857)])\n",
            "('may', [(1, 3, 0.21428571428571427)])\n",
            "('dec', [(1, 6, 0.42857142857142855)])\n",
            "('destinations', [(1, 1, 0.07142857142857142)])\n",
            "('covertible', [(1, 1, 0.07142857142857142)])\n",
            "('areas', [(1, 2, 0.14285714285714285)])\n",
            "('uruguay', [(1, 2, 0.14285714285714285)])\n",
            "198 inv_idx/part-00000\n",
            "198 inv_idx/part-00001\n",
            "396 /content/drive/MyDrive/inv_idx.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Islv0-ONjyF1",
        "outputId": "1f5a4745-58dd-4112-e17a-d624d3b093c6"
      },
      "source": [
        "!ls -lrt doc_mag\n",
        "!head doc_mag/part-00000\n",
        "!wc -l doc_mag/part-00000\n",
        "!wc -l doc_mag/part-00001\n",
        "!cat doc_mag/part-00000 doc_mag/part-00001 > /content/drive/MyDrive/doc_mag.txt\n",
        "!wc -l /content/drive/MyDrive/doc_mag.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 8\n",
            "-rw-r--r-- 1 root root 59 Apr  8 17:01 part-00000\n",
            "-rw-r--r-- 1 root root 86 Apr  8 17:01 part-00001\n",
            "-rw-r--r-- 1 root root  0 Apr  8 17:01 _SUCCESS\n",
            "(2, (5, 3.7600000000000016))\n",
            "(4, (14, 2.8010204081632613))\n",
            "2 doc_mag/part-00000\n",
            "3 doc_mag/part-00001\n",
            "5 /content/drive/MyDrive/doc_mag.txt\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}